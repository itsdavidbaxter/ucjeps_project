Very terse deployment instructions for this webapp.

BMU has two components:

* A django webapp that allows users to upload media (via the
  "standard" Django file upload compents) and metadata (via a
  combination of form input and metadata in the EXIF data in the
  image)

* A batch component consisting of shell and python scripts, which take
  the images and metadata and "ingest" them into CSpace.

To deploy it on an IS&T Unix Team-managed server, (A RHEL6 VM, e.g. cspace-prod.cspace.berkeley.edu):

WEBAPP COMPONENT:

* Clone this repo and install and configure the Django
  webapp(s), including this one, "as usual".  In particular, create the temporary cache,
  with appropriate permissions, and record the full pathin the deployed
  config file, uploadmedia.cfg

  e.g.

  [files]
  directory         = /tmp/image_upload_cache_pahma

BATCH COMPONENT:

* Follow the following steps to configure the batch component to run in
  the environment you are using: the batch component relies on cron or
  something similar to run, and has a number of expectations (dependencies)
  about where things are located and what they are named.
  Sorry it is so complicated, someday we'll make it more robust.

* Copy the batchmediauploadDev.cfg file to a new config file in the same directory
  (i.e. uploadmedia/) and edit the configuration for use by the the batch script postblob.sh,
  below.

  e.g.

  cp batchmediauploadDev.cfg pahma_Uploadmedia_Dev.cfg
  vi pahma_Uploadmedia_Dev.cfg

* Copy one of postblobviaFile.sh or postblobviaHttp.sh to postblob.sh
  in this directory.

  cp postblobviaHttp.sh postblob.sh

* (NB: use the former of these if this webapp is running on the same server
  as the CSpace deployment it address -- and therefore can use the
  somewhat more efficient "direct file move" CSpace blob upload
  facility). Use the latter if the blob must be POSTed over HTTP to a
  remote CSpace server.

* Edit the parameters in the beginning of this script so that the
  script will run correctly (i.e. passwords, servers, etc. etc.). Specify
  the .cfg file you edited above in MEDIACONFIG.

* Ensure that the temporary cache for images and intermediate files is
  present and properly configured with appropriate permissions. For the
  online portion, the current setting is /tmp/image_upload_cache_{TENANT}.

* Alas, there is no way to test this webapp except to upload some
  media files.

* There are some helper scripts: "runJob.sh" runs a single job from
  the command line. "runHelpers.sh" attempts to find problem media and
  create jobs to re-load/re-link them. See below.

* The batch portion of the system is run via cron, and apache owns the
  uploaded files so it has to be run as use apache.  And there is a
  script that reports on the status of uploads that runs via cron as
  well.

  Here is the current crontab for app_webapps on webapps-dev.csacpe and webapps.cspace, where this
  webapp runs nightly:

[jblowe@pahma-dev ~]$ sudo crontab -u apache -l

10  5  * * * perl /var/www/pahma/uploadmedia/checkRuns.pl jobs | expand -12 | mail -s "recent BMU jobs" pahma-cspace@berkeley.edu > /dev/null 2>&1
59 21  * * * shopt -s nullglob; for f in /tmp/upload_cache/*.step1.csv; do f=$(echo $f | sed -e 's/\.step1.csv//'); echo  Starting `date` $f >> /tmp/upload_cache/batches.log ; time /var/www/pahma/uploadmedia/postblob.sh $f >> /tmp/upload_cache/batches.log ; echo  Ending `date` $f >> /tmp/upload_cache/batches.log ; done > /dev/null 2>&1

A few more details on the batch process:

The "batch process", which runs nightly as a cron job as shown above,
is really just a small bash script wrapper around a python script
(uploadMedia.py) that does the heavy lifting. The heavy lifting is as
follows:

* The webapp creates metadata files (*.step1.csv) in a tmp/ directory.
  These point to the media files that were uploaded via the webapp to
  the same tmp/ directory. (the directory is a config parameter to the
  webapps, and for PAHMA it currently points to
  dev.space:/tmp/image_upload)

* Every night, cron globs all the *.step1.csv files together, POSTs
  the images to the Blobs service via cURL, and then passes the blob
  info and metadata to uploadMedia.py, which POSTs to the Media and
  Relations services to create Media records and connect them to the
  corresponding Collectionobjects.

* In more detail:

- the shell script postblobs.sh reads a .step1.csv file containing the
  media file name and metadata from the image and the user and POSTs
  to the Blobs service.

- for each of the Blob POSTs which succeeds (i.e. a Blob is created),
  a record is created in a file called *.step2.csv which contains the
  BlobCSID and MediaCSID, etc.

- the python script uploadMedia.py reads .step2.csv and POSTs to the
  Media service, outputing a file *.step3.csv for every successful
  Media and bidirectional Relation created.

- Finally the bash script renames the *.step*.csv file to
  *.original.csv and *.processed.csv. This creates a trace for
  verification and prevents the images from being reprocessed the next
  time the cron job runs.

As an aside, note that this tmp/ directory is cleaned up by the OS,
which deletes files older than 10 days -- i.e. there is a 10 day cache
of images load, in case something needs to be recovered or rerun.

Finally, there are a few other (very speculative!) scripts that are
used for reporting and maintenance:

* bulkmediaupload.sh - this runs a single *.step1.csv file; it is used
by the webapp to do "online uploads"

* checkObj.pl - this script checks Blobs uploaded vs. Media created; it
is used by the very rickety "runHelpers.sh" script below to report on
problem records.

* checkRuns.pl - this script creates some reports on BMU activity:
number of jobs and their status, lists of images and the CSIDs, finds
problem data (missing images, duplicates, etc.)

* runHelpers.sh - matches CSIDs loaded with log files, to find
discrepancies between what is in the database and what seems to have
been uploaded. No longer works due to schema changes in the Solr
datasource, which it uses to find Blob CSIDs.

* runJob.sh - like bulkmediaupload.sh, but does not cleanup/rename
intermediate files.

* verifyObjectsAndMedia.sh - somewhat scary script to make a list of
imagefile names and object numbers...

RUNNING THE BMU FROM THE COMMAND LINE

Once it is installed, the BMU can be invoked on the command line as:

$ bulkmediaupload.sh xxxxxx

Provided you have a metadata file named "xxxxxx.step1.csv" in the pwd with the following fields (the header is required):

name (i.e. field name)
size (can be null)
objectnumber
date (can be null)
creator (refname)
contributor
rightsholder (refname)

e.g.

1-4673_2014_10_13at09_18_25.JPG

4416952

1-4673

2014:10:13 09:18:25

urn:cspace:pahma.cspace.berkeley.edu:personauthorities:name(person):item:name(7652)'A Person Name'

Trinity Miller

urn:cspace:pahma.cspace.berkeley.edu:orgauthorities:name(organization):item:name(8107)'Phoebe A. Hearst Museum of Anthropology'

NB: the image files mentioned in this file must also be present in this same directory.
